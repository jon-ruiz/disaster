{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models of disaster tweet categories using Hurricanes Harvey, Irma, and Maria training data\n",
    "  -- Several model types tested\n",
    "  \n",
    "  \n",
    "  -- Use best model to predict categories for Hurricane Michael tweets (out-of-event sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pauls\\Anaconda3\\envs\\dsi\\lib\\site-packages\\smart_open\\ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report,classification\n",
    "from sklearn.ensemble import BaggingClassifier,RandomForestClassifier,ExtraTreesClassifier,AdaBoostClassifier,GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from nltk import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.test.utils import common_dictionary, common_corpus\n",
    "from gensim.models import LsiModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read pickled training data for 3 hurricanes\n",
    "tweets = pd.read_pickle(\"../data/train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.491601\n",
       "2    0.278394\n",
       "3    0.095827\n",
       "4    0.075753\n",
       "5    0.034760\n",
       "6    0.016799\n",
       "7    0.005283\n",
       "8    0.001585\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#baseline accuracy\n",
    "tweets.y.value_counts(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text_info</th>\n",
       "      <th>text_human</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>event</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>905274232590004225</td>\n",
       "      <td>not_informative</td>\n",
       "      <td>not_relevant_or_cant_judge</td>\n",
       "      <td>CONGRATS ON HITTING YOIR GOAL GUYS, I'm sure t...</td>\n",
       "      <td>harvey</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>901646074527535105</td>\n",
       "      <td>informative</td>\n",
       "      <td>injured_or_dead_people</td>\n",
       "      <td>RT @ajwamood: #ajwamood : Harvey the first maj...</td>\n",
       "      <td>harvey</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>901646123080830976</td>\n",
       "      <td>informative</td>\n",
       "      <td>other_relevant_information</td>\n",
       "      <td>RT @yIIeza: When we get back to SCHS after Har...</td>\n",
       "      <td>harvey</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>901646127895863296</td>\n",
       "      <td>informative</td>\n",
       "      <td>other_relevant_information</td>\n",
       "      <td>Not always good when your city shows up on a s...</td>\n",
       "      <td>harvey</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>901646131628830721</td>\n",
       "      <td>informative</td>\n",
       "      <td>other_relevant_information</td>\n",
       "      <td>RT @MSNBC: Side by side satellite images compa...</td>\n",
       "      <td>harvey</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweet_id        text_info                  text_human  \\\n",
       "0   905274232590004225  not_informative  not_relevant_or_cant_judge   \n",
       "1   901646074527535105      informative      injured_or_dead_people   \n",
       "8   901646123080830976      informative  other_relevant_information   \n",
       "9   901646127895863296      informative  other_relevant_information   \n",
       "10  901646131628830721      informative  other_relevant_information   \n",
       "\n",
       "                                           tweet_text   event  y  \n",
       "0   CONGRATS ON HITTING YOIR GOAL GUYS, I'm sure t...  harvey  4  \n",
       "1   RT @ajwamood: #ajwamood : Harvey the first maj...  harvey  6  \n",
       "8   RT @yIIeza: When we get back to SCHS after Har...  harvey  1  \n",
       "9   Not always good when your city shows up on a s...  harvey  1  \n",
       "10  RT @MSNBC: Side by side satellite images compa...  harvey  1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(col):\n",
    "    # convert text to lower case\n",
    "    col = col.str.lower()\n",
    "\n",
    "    # remove URLs\n",
    "    col = col.apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
    "    col = col.apply(lambda x: re.split('http:\\/\\/.*', str(x))[0])\n",
    "    col = col.replace(r'www\\S+', '', regex=True)\n",
    "\n",
    "    #remove \"RT\" string\n",
    "    col = col.map(lambda x: x.lstrip('rt'))\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['clean_text'] = clean_tweets(tweets['tweet_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Name variables \n",
    "X = tweets[[\"clean_text\"]]\n",
    "y = tweets[\"y\"]\n",
    "\n",
    "#train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stopwords from NLTK and add custom words\n",
    "mystopwords = stopwords.words('english')\n",
    "mystopwords.extend(['hurricane','tornado','harvey','irma','joplin','sandy','maria',\n",
    "                    'like','would','get','x200b','https','one','www','com','org','etc','could'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes model with Count Vectorizer\n",
      "Train data accuracy: 0.774\n",
      "Test data accuracy: 0.676\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes with CVEC\n",
    "tknzr = TweetTokenizer()\n",
    "cvec = CountVectorizer(tokenizer=tknzr.tokenize,stop_words=mystopwords,\n",
    "                       max_features=3000,max_df=1.0,min_df=2, \n",
    "                       ngram_range=(1,1))\n",
    "\n",
    "# Fit  CountVectorizer on the training data and transform training data.\n",
    "X_train_cvec = pd.DataFrame(cvec.fit_transform(X_train['clean_text']).todense(),\n",
    "                            columns = cvec.get_feature_names())\n",
    "# Transform our testing data with the already-fit CountVectorizer.\n",
    "X_test_cvec = pd.DataFrame(cvec.transform(X_test['clean_text']).todense(),\n",
    "                           columns = cvec.get_feature_names())\n",
    "\n",
    "# instantiate and fit model \n",
    "nb = MultinomialNB(alpha=1)\n",
    "nb.fit(X_train_cvec, y_train)\n",
    "\n",
    "# Score model on the training set.\n",
    "print('Naive Bayes model with Count Vectorizer')\n",
    "print(f'Train data accuracy: {nb.score(X_train_cvec,y_train):.3f}')\n",
    "\n",
    "# Score our model on the testing set.\n",
    "print(f'Test data accuracy: {nb.score(X_test_cvec,y_test):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes model with TF-IDF Vectorizer\n",
      "Train data accuracy: 0.702\n",
      "Test data accuracy: 0.662\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes Bernouilli with Tfidf\n",
    "tvec = TfidfVectorizer(tokenizer=tknzr.tokenize,stop_words=None,max_features=3000,max_df=1.0,min_df=2, ngram_range=(1,1))\n",
    "\n",
    "# Fit our CountVectorizer on the training data and transform training data.\n",
    "X_train_tvec = pd.DataFrame(tvec.fit_transform(X_train['clean_text']).todense(),\n",
    "                            columns = tvec.get_feature_names())\n",
    "# Transform our testing data with the already-fit CountVectorizer.\n",
    "X_test_tvec = pd.DataFrame(tvec.transform(X_test['clean_text']).todense(),\n",
    "                           columns = tvec.get_feature_names())\n",
    "\n",
    "# instantiate and fit model \n",
    "nb = MultinomialNB(alpha=1)\n",
    "nb.fit(X_train_tvec, y_train)\n",
    "\n",
    "# Score model on the training set.\n",
    "print('Naive Bayes model with TF-IDF Vectorizer')\n",
    "print(f'Train data accuracy: {nb.score(X_train_tvec,y_train):.3f}')\n",
    "\n",
    "# Score our model on the testing set.\n",
    "print(f'Test data accuracy: {nb.score(X_test_tvec,y_test):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine model with Count Vectorizer\n",
      "Train data accuracy: 0.884\n",
      "Test data accuracy: 0.687\n"
     ]
    }
   ],
   "source": [
    "# SVM model with CVEC\n",
    "cvec = CountVectorizer(stop_words=mystopwords,max_features=800, ngram_range=(1,1))\n",
    "\n",
    "# Fit  CountVectorizer on the training data and transform training data.\n",
    "X_train_cvec = pd.DataFrame(cvec.fit_transform(X_train['clean_text']).todense(),\n",
    "                            columns = cvec.get_feature_names())\n",
    "# Transform our testing data with the already-fit CountVectorizer.\n",
    "X_test_cvec = pd.DataFrame(cvec.transform(X_test['clean_text']).todense(),\n",
    "                           columns = cvec.get_feature_names())\n",
    "# Instantiate SVM.\n",
    "# svc = svm.SVC(kernel='poly', C = 1.8, gamma = .05) \n",
    "svc = svm.SVC(kernel='rbf', C = 2, gamma = .2)  \n",
    "\n",
    "# Fit on training data.\n",
    "svc.fit(X_train_cvec,y_train)\n",
    "\n",
    "# Score model on the training set.\n",
    "print('Support Vector Machine model with Count Vectorizer')\n",
    "print(f'Train data accuracy: {svc.score(X_train_cvec,y_train):.3f}')\n",
    "\n",
    "# Score our model on the testing set.\n",
    "print(f'Test data accuracy: {svc.score(X_test_cvec,y_test):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression model with Count Vectorizer and TweetTokenzer\n",
      "Train data accuracy: 0.957\n",
      "Test data accuracy: 0.706\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression with CountVectorizer \n",
    "tknzr = TweetTokenizer()\n",
    "cvec = CountVectorizer(stop_words=\"english\", tokenizer=tknzr.tokenize, ngram_range=(1,1))\n",
    "X_train_cvec = pd.DataFrame(cvec.fit_transform(X_train['clean_text']).todense(), columns = cvec.get_feature_names())\n",
    "X_test_cvec = pd.DataFrame(cvec.transform(X_test['clean_text']).todense(), columns = cvec.get_feature_names())\n",
    "lr = LogisticRegression(penalty='l2',C=1.5,random_state=42, solver='liblinear', multi_class='ovr')\n",
    "lr_model = lr.fit(X_train_cvec, y_train)\n",
    "\n",
    "# Score model on the training set.\n",
    "print('Logistic Regression model with Count Vectorizer and TweetTokenzer')\n",
    "print(f'Train data accuracy: {lr.score(X_train_cvec,y_train):.3f}')\n",
    "\n",
    "# Score our model on the testing set.\n",
    "print(f'Test data accuracy: {lr.score(X_test_cvec,y_test):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training R-sq is: 0.587\n",
      "Testing R-sq is: 0.573\n"
     ]
    }
   ],
   "source": [
    "# random forest\n",
    "model = RandomForestClassifier(max_depth=20,random_state=42,min_samples_leaf=5,n_estimators=10)\n",
    "model.fit(X_train_cvec,y_train)\n",
    "y_pred = model.predict(X_test_cvec)\n",
    "y_pred_train = model.predict(X_train_cvec)\n",
    "\n",
    "print(f'Training R-sq is: {model.score(X_train_cvec,y_train):.3f}')\n",
    "print(f'Testing R-sq is: {model.score(X_test_cvec,y_test):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Results of Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>other_relevant_information</th>\n",
       "      <td>0.866838</td>\n",
       "      <td>0.079038</td>\n",
       "      <td>0.032646</td>\n",
       "      <td>0.018041</td>\n",
       "      <td>0.003436</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rescue_volunteering_or_donation_effort</th>\n",
       "      <td>0.171472</td>\n",
       "      <td>0.798179</td>\n",
       "      <td>0.010622</td>\n",
       "      <td>0.013657</td>\n",
       "      <td>0.006070</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>infrastructure_and_utility_damage</th>\n",
       "      <td>0.568282</td>\n",
       "      <td>0.039648</td>\n",
       "      <td>0.378855</td>\n",
       "      <td>0.008811</td>\n",
       "      <td>0.004405</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not_relevant_or_cant_judge</th>\n",
       "      <td>0.754190</td>\n",
       "      <td>0.100559</td>\n",
       "      <td>0.027933</td>\n",
       "      <td>0.106145</td>\n",
       "      <td>0.011173</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>affected_individuals</th>\n",
       "      <td>0.597561</td>\n",
       "      <td>0.292683</td>\n",
       "      <td>0.036585</td>\n",
       "      <td>0.012195</td>\n",
       "      <td>0.060976</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>injured_or_dead_people</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vehicle_damage</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missing_or_found_people</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               0         1         2  \\\n",
       "other_relevant_information              0.866838  0.079038  0.032646   \n",
       "rescue_volunteering_or_donation_effort  0.171472  0.798179  0.010622   \n",
       "infrastructure_and_utility_damage       0.568282  0.039648  0.378855   \n",
       "not_relevant_or_cant_judge              0.754190  0.100559  0.027933   \n",
       "affected_individuals                    0.597561  0.292683  0.036585   \n",
       "injured_or_dead_people                  0.250000  0.125000  0.000000   \n",
       "vehicle_damage                          0.750000  0.000000  0.083333   \n",
       "missing_or_found_people                 1.000000  0.000000  0.000000   \n",
       "\n",
       "                                               3         4      5         6  \\\n",
       "other_relevant_information              0.018041  0.003436  0.000  0.000000   \n",
       "rescue_volunteering_or_donation_effort  0.013657  0.006070  0.000  0.000000   \n",
       "infrastructure_and_utility_damage       0.008811  0.004405  0.000  0.000000   \n",
       "not_relevant_or_cant_judge              0.106145  0.011173  0.000  0.000000   \n",
       "affected_individuals                    0.012195  0.060976  0.000  0.000000   \n",
       "injured_or_dead_people                  0.000000  0.050000  0.575  0.000000   \n",
       "vehicle_damage                          0.000000  0.000000  0.000  0.166667   \n",
       "missing_or_found_people                 0.000000  0.000000  0.000  0.000000   \n",
       "\n",
       "                                          7  \n",
       "other_relevant_information              0.0  \n",
       "rescue_volunteering_or_donation_effort  0.0  \n",
       "infrastructure_and_utility_damage       0.0  \n",
       "not_relevant_or_cant_judge              0.0  \n",
       "affected_individuals                    0.0  \n",
       "injured_or_dead_people                  0.0  \n",
       "vehicle_damage                          0.0  \n",
       "missing_or_found_people                 0.0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "# source: https://stackoverflow.com/questions/39770376/scikit-learn-get-accuracy-scores-for-each-class\n",
    "y_pred = lr.predict(X_test_cvec)\n",
    "\n",
    "#Get the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# normalize the diagonal entries\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "#The diagonal entries are the accuracies of each class\n",
    "cm.diagonal()\n",
    "\n",
    "categories = [\"other_relevant_information\",\"rescue_volunteering_or_donation_effort\", \n",
    "     \"infrastructure_and_utility_damage\", \"not_relevant_or_cant_judge\",\n",
    "     \"affected_individuals\", 'injured_or_dead_people', 'vehicle_damage',\n",
    "     'missing_or_found_people']\n",
    "cm_df = pd.DataFrame(data=cm, index=categories,columns=None)\n",
    "cm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use model to make category predictions for Hurricane Michael tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "michael = pd.read_pickle('../data/hurricane_michael.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50043, 11)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "michael.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>favorites</th>\n",
       "      <th>geo</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>id</th>\n",
       "      <th>mentions</th>\n",
       "      <th>permalink</th>\n",
       "      <th>retweets</th>\n",
       "      <th>text</th>\n",
       "      <th>to</th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-10-12 23:59:57+00:00</td>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td>#HurricaneMichael #blessed #UnitedWeStand #tal...</td>\n",
       "      <td>1050898900582838272</td>\n",
       "      <td>@COTNews</td>\n",
       "      <td>https://twitter.com/joeearenas/status/10508989...</td>\n",
       "      <td>0</td>\n",
       "      <td>@COTNews has been working 24/7 to restore serv...</td>\n",
       "      <td>None</td>\n",
       "      <td>joeearenas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-10-12 23:59:53+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1050898882526371842</td>\n",
       "      <td></td>\n",
       "      <td>https://twitter.com/LakesideBexley/status/1050...</td>\n",
       "      <td>0</td>\n",
       "      <td>In the wake of Hurricane Michael, we understan...</td>\n",
       "      <td>None</td>\n",
       "      <td>LakesideBexley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-10-12 23:59:52+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>#HurricaneMichael #Florida</td>\n",
       "      <td>1050898882077442048</td>\n",
       "      <td></td>\n",
       "      <td>https://twitter.com/PRAISETRIUNEGOD/status/105...</td>\n",
       "      <td>0</td>\n",
       "      <td>Maybe 17 \" #HurricaneMichael Updates: Body Fou...</td>\n",
       "      <td>None</td>\n",
       "      <td>PRAISETRIUNEGOD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-10-12 23:59:49+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1050898865988222976</td>\n",
       "      <td></td>\n",
       "      <td>https://twitter.com/aShartee/status/1050898865...</td>\n",
       "      <td>0</td>\n",
       "      <td>In other news praying for those affected by hu...</td>\n",
       "      <td>None</td>\n",
       "      <td>aShartee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-10-12 23:59:43+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>#HurricaneMichael #ExcessiveForce</td>\n",
       "      <td>1050898841879236608</td>\n",
       "      <td></td>\n",
       "      <td>https://twitter.com/MindOfMo/status/1050898841...</td>\n",
       "      <td>0</td>\n",
       "      <td>iSpy 2 or 3 who'd be hard-pressed to RUN in an...</td>\n",
       "      <td>CBSNews</td>\n",
       "      <td>MindOfMo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date  favorites geo  \\\n",
       "0 2018-10-12 23:59:57+00:00          4       \n",
       "1 2018-10-12 23:59:53+00:00          1       \n",
       "2 2018-10-12 23:59:52+00:00          0       \n",
       "3 2018-10-12 23:59:49+00:00          0       \n",
       "4 2018-10-12 23:59:43+00:00          0       \n",
       "\n",
       "                                            hashtags                   id  \\\n",
       "0  #HurricaneMichael #blessed #UnitedWeStand #tal...  1050898900582838272   \n",
       "1                                                     1050898882526371842   \n",
       "2                         #HurricaneMichael #Florida  1050898882077442048   \n",
       "3                                                     1050898865988222976   \n",
       "4                  #HurricaneMichael #ExcessiveForce  1050898841879236608   \n",
       "\n",
       "   mentions                                          permalink  retweets  \\\n",
       "0  @COTNews  https://twitter.com/joeearenas/status/10508989...         0   \n",
       "1            https://twitter.com/LakesideBexley/status/1050...         0   \n",
       "2            https://twitter.com/PRAISETRIUNEGOD/status/105...         0   \n",
       "3            https://twitter.com/aShartee/status/1050898865...         0   \n",
       "4            https://twitter.com/MindOfMo/status/1050898841...         0   \n",
       "\n",
       "                                                text       to         username  \n",
       "0  @COTNews has been working 24/7 to restore serv...     None       joeearenas  \n",
       "1  In the wake of Hurricane Michael, we understan...     None   LakesideBexley  \n",
       "2  Maybe 17 \" #HurricaneMichael Updates: Body Fou...     None  PRAISETRIUNEGOD  \n",
       "3  In other news praying for those affected by hu...     None         aShartee  \n",
       "4  iSpy 2 or 3 who'd be hard-pressed to RUN in an...  CBSNews         MindOfMo  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "michael.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean tweets using previously defined function\n",
    "michael['clean_text'] = clean_tweets(michael['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform out-of-event testing data with the already-fit CountVectorizer.\n",
    "michael_cvec = pd.DataFrame(cvec.transform(michael['clean_text']).todense(),\n",
    "                           columns = cvec.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "michael['pred'] = lr_model.predict(michael_cvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.512759\n",
       "2    0.213756\n",
       "3    0.160462\n",
       "6    0.057371\n",
       "4    0.037228\n",
       "5    0.018224\n",
       "7    0.000200\n",
       "Name: inform_pred, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "michael.pred.value_counts(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  {\"other_relevant_information\":1,\"rescue_volunteering_or_donation_effort\":2, \n",
    "#      \"infrastructure_and_utility_damage\":3, \"not_relevant_or_cant_judge\":4,\n",
    "#      \"affected_individuals\":5, 'injured_or_dead_people':6, 'vehicle_damage':7,\n",
    "#      'missing_or_found_people':8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cars, boats carried away by Hurricane Michael, reports @MikeMagsCBS12 http://bit.ly/2QKPqvz pic.twitter.com/LfEKVgca0z\n",
      "---------------------------------\n",
      "@Honda thank you for building such safe and reliable cars. We rode out Hurricane Michael in a Civic, survived without a scratch, and I'm happy to say we're driving it back to Texas right now. This car needs to be in a museum. pic.twitter.com/xU0aqNs5xl\n",
      "---------------------------------\n",
      "All those flooded cars in #Florida? @Ford will be making up the $1B Trump has cost them when people buy their new cars. #HurricaneMichael http://fortune.com/2018/10/09/ford-stock-today-layoffs-trump-trade-tariffs/ …\n",
      "---------------------------------\n",
      "Rail cars flipped on their sides from the force of #HurricaneMichael in Panama City @winknewspic.twitter.com/SsCbObMNQq\n",
      "---------------------------------\n",
      "literally sitting in my car, listening to 90.5, charging my phone... must not be the only one, cause I see 2 other cars sitting too #noelectricity #HurricaneMichael #Tallahassee\n",
      "---------------------------------\n",
      "#BREAKING Thankfully a @CCPDVa Officer was here in Powhatan - he just helped a woman out of her stranded car. More than a dozen of us are stuck here at a gas station because of flooded roads #HurricaneMichael @CBS6pic.twitter.com/0ERxZ8kADV\n",
      "---------------------------------\n",
      "The winds were so strong they flipped all of these rail cars on their side along Highway 231. #HurricaneMichael #BayCountyFL @FCN2go https://www.instagram.com/p/Boz09PKgf8m/?utm_source=ig_twitter_share&igshid=12sxf8yv0atl2 …\n",
      "---------------------------------\n",
      "The winds were so strong they flipped all of these rail cars on their side along Highway 231 in Bay County. #HurricaneMichael @FCN2gopic.twitter.com/fcgVQP7SfV\n",
      "---------------------------------\n",
      "My brother rode out #HurricaneMichael in a trailer and lived to tell the tale. Pics below. Trailer and car miraculously intact but no trees left on this property at all. Devastated for my hometown pic.twitter.com/PFnwRFWNDW\n",
      "---------------------------------\n",
      "Video Shows Classic Cars, Garage Ravaged By Hurricane Michael: Given the devastation of this historic storm, we suspect these cars were the lucky ones. http://www.automotivetestdrivers.com/classic-cars-garage-hurricane-michael/?utm_source=dlvr.it&utm_medium=twitter … #Cars #Autos #Automotive\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "mask = michael.inform_pred==7\n",
    "for tweet in michael.text[mask]:\n",
    "    print(tweet)\n",
    "    print('---------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
