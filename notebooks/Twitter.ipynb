{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GetOldTweets3 as got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occured during an HTTP request: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "Try to open in browser: https://twitter.com/search?q=hurricane%20michael%20since%3A2018-10-09%20until%3A2018-10-12&src=typd\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pauls\\Anaconda3\\envs\\dsi\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2971: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "tweetCriteria = got.manager.TweetCriteria().setQuerySearch('hurricane michael')\\\n",
    "                                           .setSince(\"2018-10-09\")\\\n",
    "                                           .setUntil(\"2018-10-12\")\\\n",
    "                                           .setMaxTweets(1000000)\n",
    "\n",
    "tweets = got.manager.TweetManager.getTweets(tweetCriteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Much going on in the world especially in US need prayers for all states being in route of Hurricane Michael please continue to pray support give kindness love compassion only necessities #food #clothes #shelterlove @marceecornpic.twitter.com/grVSmrIljK'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_list = []\n",
    "for i in range(len(tweets)):\n",
    "    t_list.append({\n",
    "       'id': tweets[i].id,\n",
    "        'permalink': tweets[i].permalink,\n",
    "        'username' : tweets[i].username,\n",
    "        'to': tweets[i].to,\n",
    "        'text': tweets[i].text,\n",
    "        'date' : tweets[i].date,\n",
    "        'retweets' : tweets[i].retweets,\n",
    "        'favorites' : tweets[i].favorites,\n",
    "        'mentions' : tweets[i].mentions,\n",
    "        'hashtags' : tweets[i].hashtags,\n",
    "        'geo': tweets[i].geo \n",
    "    })\n",
    "df = pd.DataFrame(t_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://twitter.com/Harrison_PRS/status/1050793365225246720'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.permalink[10101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('hurricane_michael_Oct11_p1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 11)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = pd.concat([df,firstbatch],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63681, 11)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged.to_pickle('hurricane_michael.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        31407\n",
       "1         7154\n",
       "2         3073\n",
       "3         1767\n",
       "4         1139\n",
       "5          742\n",
       "6          589\n",
       "7          444\n",
       "8          328\n",
       "9          279\n",
       "10         253\n",
       "11         202\n",
       "12         174\n",
       "13         167\n",
       "14         127\n",
       "15          99\n",
       "16          97\n",
       "17          80\n",
       "19          78\n",
       "18          77\n",
       "20          69\n",
       "23          63\n",
       "22          62\n",
       "21          58\n",
       "24          48\n",
       "26          45\n",
       "27          42\n",
       "25          41\n",
       "29          34\n",
       "28          32\n",
       "         ...  \n",
       "1781         1\n",
       "1269         1\n",
       "981          1\n",
       "565          1\n",
       "341          1\n",
       "277          1\n",
       "245          1\n",
       "181          1\n",
       "2164         1\n",
       "22078        1\n",
       "500          1\n",
       "372          1\n",
       "308          1\n",
       "947          1\n",
       "723          1\n",
       "691          1\n",
       "435          1\n",
       "243          1\n",
       "179          1\n",
       "8017         1\n",
       "1330         1\n",
       "274          1\n",
       "242          1\n",
       "178          1\n",
       "1489         1\n",
       "1393         1\n",
       "913          1\n",
       "529          1\n",
       "433          1\n",
       "591          1\n",
       "Name: favorites, Length: 387, dtype: int64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.favorites.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0       2018-10-12 23:59:57+00:00\n1       2018-10-12 23:59:53+00:00\n2       2018-10-12 23:59:52+00:00\n3       2018-10-12 23:59:49+00:00\n4       2018-10-12 23:59:43+00:00\n5       2018-10-12 23:59:42+00:00\n6       2018-10-12 23:59:37+00:00\n7       2018-10-12 23:59:35+00:00\n8       2018-10-12 23:59:25+00:00\n9       2018-10-12 23:59:24+00:00\n10      2018-10-12 23:59:24+00:00\n11      2018-10-12 23:59:22+00:00\n12      2018-10-12 23:59:22+00:00\n13      2018-10-12 23:59:20+00:00\n14      2018-10-12 23:59:18+00:00\n15      2018-10-12 23:59:11+00:00\n16      2018-10-12 23:58:56+00:00\n17      2018-10-12 23:58:56+00:00\n18      2018-10-12 23:58:53+00:00\n19      2018-10-12 23:58:53+00:00\n20      2018-10-12 23:58:49+00:00\n21      2018-10-12 23:58:37+00:00\n22      2018-10-12 23:58:36+00:00\n23      2018-10-12 23:58:26+00:00\n24      2018-10-12 23:58:25+00:00\n25      2018-10-12 23:58:12+00:00\n26      2018-10-12 23:58:12+00:00\n27      2018-10-12 23:58:03+00:00\n28      2018-10-12 23:58:01+00:00\n29      2018-10-12 23:58:00+00:00\n                   ...           \n3415    2018-10-12 16:47:08+00:00\n3463    2018-10-12 16:42:04+00:00\n3485    2018-10-12 16:39:00+00:00\n3491    2018-10-12 16:37:59+00:00\n3961    2018-10-12 15:47:21+00:00\n4570    2018-10-12 14:49:56+00:00\n4606    2018-10-12 14:46:38+00:00\n4690    2018-10-12 14:39:00+00:00\n5665    2018-10-12 13:09:41+00:00\n5902    2018-10-12 12:45:02+00:00\n6973    2018-10-12 09:10:09+00:00\n7095    2018-10-12 08:16:47+00:00\n7139    2018-10-12 07:47:07+00:00\n7147    2018-10-12 07:41:50+00:00\n7152    2018-10-12 07:40:05+00:00\n7175    2018-10-12 07:26:28+00:00\n7349    2018-10-12 06:31:37+00:00\n7742    2018-10-12 04:17:37+00:00\n8141    2018-10-12 03:03:10+00:00\n8423    2018-10-12 02:20:40+00:00\n10397   2018-10-11 23:19:58+00:00\n11400   2018-10-11 22:10:21+00:00\n11715   2018-10-11 21:47:39+00:00\n12086   2018-10-11 21:24:24+00:00\n12134   2018-10-11 21:20:39+00:00\n12387   2018-10-11 21:01:14+00:00\n13107   2018-10-11 20:08:35+00:00\n13194   2018-10-11 20:03:43+00:00\n13307   2018-10-11 19:57:43+00:00\n13314   2018-10-11 19:57:13+00:00\nName: date, Length: 50043, dtype: datetime64[ns, UTC]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-99f5d31cb4cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmerged\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmerged\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\dsi\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36msort_values\u001b[1;34m(self, by, axis, ascending, inplace, kind, na_position)\u001b[0m\n\u001b[0;32m   4416\u001b[0m             \u001b[0mby\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4417\u001b[0m             k = self._get_label_or_level_values(by, axis=axis,\n\u001b[1;32m-> 4418\u001b[1;33m                                                 stacklevel=stacklevel)\n\u001b[0m\u001b[0;32m   4419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4420\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dsi\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_label_or_level_values\u001b[1;34m(self, key, axis, stacklevel)\u001b[0m\n\u001b[0;32m   1377\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1378\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1379\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1381\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0       2018-10-12 23:59:57+00:00\n1       2018-10-12 23:59:53+00:00\n2       2018-10-12 23:59:52+00:00\n3       2018-10-12 23:59:49+00:00\n4       2018-10-12 23:59:43+00:00\n5       2018-10-12 23:59:42+00:00\n6       2018-10-12 23:59:37+00:00\n7       2018-10-12 23:59:35+00:00\n8       2018-10-12 23:59:25+00:00\n9       2018-10-12 23:59:24+00:00\n10      2018-10-12 23:59:24+00:00\n11      2018-10-12 23:59:22+00:00\n12      2018-10-12 23:59:22+00:00\n13      2018-10-12 23:59:20+00:00\n14      2018-10-12 23:59:18+00:00\n15      2018-10-12 23:59:11+00:00\n16      2018-10-12 23:58:56+00:00\n17      2018-10-12 23:58:56+00:00\n18      2018-10-12 23:58:53+00:00\n19      2018-10-12 23:58:53+00:00\n20      2018-10-12 23:58:49+00:00\n21      2018-10-12 23:58:37+00:00\n22      2018-10-12 23:58:36+00:00\n23      2018-10-12 23:58:26+00:00\n24      2018-10-12 23:58:25+00:00\n25      2018-10-12 23:58:12+00:00\n26      2018-10-12 23:58:12+00:00\n27      2018-10-12 23:58:03+00:00\n28      2018-10-12 23:58:01+00:00\n29      2018-10-12 23:58:00+00:00\n                   ...           \n3415    2018-10-12 16:47:08+00:00\n3463    2018-10-12 16:42:04+00:00\n3485    2018-10-12 16:39:00+00:00\n3491    2018-10-12 16:37:59+00:00\n3961    2018-10-12 15:47:21+00:00\n4570    2018-10-12 14:49:56+00:00\n4606    2018-10-12 14:46:38+00:00\n4690    2018-10-12 14:39:00+00:00\n5665    2018-10-12 13:09:41+00:00\n5902    2018-10-12 12:45:02+00:00\n6973    2018-10-12 09:10:09+00:00\n7095    2018-10-12 08:16:47+00:00\n7139    2018-10-12 07:47:07+00:00\n7147    2018-10-12 07:41:50+00:00\n7152    2018-10-12 07:40:05+00:00\n7175    2018-10-12 07:26:28+00:00\n7349    2018-10-12 06:31:37+00:00\n7742    2018-10-12 04:17:37+00:00\n8141    2018-10-12 03:03:10+00:00\n8423    2018-10-12 02:20:40+00:00\n10397   2018-10-11 23:19:58+00:00\n11400   2018-10-11 22:10:21+00:00\n11715   2018-10-11 21:47:39+00:00\n12086   2018-10-11 21:24:24+00:00\n12134   2018-10-11 21:20:39+00:00\n12387   2018-10-11 21:01:14+00:00\n13107   2018-10-11 20:08:35+00:00\n13194   2018-10-11 20:03:43+00:00\n13307   2018-10-11 19:57:43+00:00\n13314   2018-10-11 19:57:13+00:00\nName: date, Length: 50043, dtype: datetime64[ns, UTC]"
     ]
    }
   ],
   "source": [
    "merged.sort_values(by=merged.date,ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twitterscraper import query_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_tweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-2-698f1504ad87>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-698f1504ad87>\"\u001b[1;36m, line \u001b[1;32m9\u001b[0m\n\u001b[1;33m    file = open(“output.txt”,”w”)\u001b[0m\n\u001b[1;37m                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    list_of_tweets = query_tweets(\"Trump OR Clinton\", limit=10, \n",
    "                                  begindate=datetime.date(2006, 3, 21), \n",
    "                                  enddate=datetime.date(2019, 4, 15), \n",
    "                                  poolsize=20, lang='en')\n",
    "    #print the retrieved tweets to the screen:\n",
    "    for tweet in query_tweets(\"Trump OR Clinton\", 10):\n",
    "        print(tweet)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs, json\n",
    "import pandas as pd\n",
    "\n",
    "with codecs.open('tweets.json', 'r', 'utf-8') as f:\n",
    "    tweets = json.load(f, encoding='utf-8')\n",
    "\n",
    "list_tweets = [list(elem.values()) for elem in tweets]\n",
    "list_columns = list(tweets[0].keys())\n",
    "df = pd.DataFrame(list_tweets, columns=list_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Or save the retrieved tweets to file:\n",
    "    file = open(“output.txt”,”w”)\n",
    "    for tweet in query_tweets(\"Trump OR Clinton\", 10):\n",
    "        file.write(tweet.encode('utf-8'))\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-h or --help Print out the help message and exits.\n",
    "-l or --limit TwitterScraper stops scraping when at least the number of tweets indicated with --limit is scraped. Since tweets are retrieved in batches of 20, this will always be a multiple of 20. Omit the limit to retrieve all tweets. You can at any time abort the scraping by pressing Ctrl+C, the scraped tweets will be stored safely in your JSON file.\n",
    "--lang Retrieves tweets written in a specific language. Currently 30+ languages are supported. For a full list of the languages print out the help message.\n",
    "-bd or --begindate Set the date from which TwitterScraper should start scraping for your query. Format is YYYY-MM-DD. The default value is set to 2006-03-21. This does not work in combination with --user.\n",
    "-ed or --enddate Set the enddate which TwitterScraper should use to stop scraping for your query. Format is YYYY-MM-DD. The default value is set to today. This does not work in combination with --user.\n",
    "-u or --user Scrapes the tweets from that users profile page. This also includes all retweets by that user. See examples below.\n",
    "-p or --poolsize Set the number of parallel processes TwitterScraper should initiate while scraping for your query. Default value is set to 20. Depending on the computational power you have, you can increase this number. It is advised to keep this number below the number of days you are scraping. For example, if you are scraping from 2017-01-10 to 2017-01-20, you can set this number to a maximum of 10. If you are scraping from 2016-01-01 to 2016-12-31, you can increase this number to a maximum of 150, if you have the computational resources. Does not work in combination with --user.\n",
    "-o or --output Gives the name of the output file. If no output filename is given, the default filename 'tweets.json' or 'tweets.csv' will be used.\n",
    "-c or --csv Write the result to a CSV file instead of a JSON file.\n",
    "-d or --dump: With this argument, the scraped tweets will be printed to the screen instead of an outputfile. If you are using this argument, the --output argument doe not need to be used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
